{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/home/ubuntu/Align-DS-V\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    low_cpu_mem_usage=True, \n",
    ").to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜User｜>What is the result of this problem?<｜Assistant｜><think>To solve the problem, I will first interpret the image to understand what mathematical operation is being represented. Then, I will perform the calculation based on the numbers provided in the image and confirm the result.\n",
      "The image shows a chalkboard with the equation \\(18 + 23 = 41\\) written on it. The numbers 18 and 23 are in light blue, and the result 41 is in light green.\n",
      "The equation \\(18 + 23 = 41\\) is presented on the chalkboard. To solve this, I will add the two numbers on the left side of the equation: 18 and 23. Adding these together, \\(18 + 23\\), I calculate that the sum is 41. This matches the number on the right side of the equation, confirming its correctness.</think>41\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Define a chat history and use `apply_chat_template` to get correctly formatted prompt\n",
    "# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\") \n",
    "conversation = [\n",
    "    {\n",
    "\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "          {\"type\": \"text\", \"text\": \"What is the result of this problem?\"},\n",
    "          {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "\n",
    "image_file = \"/home/ubuntu/Align-DS-V/assets/demo.jpg\" # in this repo\n",
    "raw_image = Image.open(image_file)\n",
    "inputs = processor(images=raw_image, text=prompt, return_tensors='pt').to(0, torch.float16)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=4096, do_sample=False)\n",
    "print(processor.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <think>To solve the problem, I will first interpret the image to understand what\n",
    "# mathematical operation is being represented. Then, I will perform the calculation\n",
    "# based on the numbers provided in the image and confirm the result. The image shows\n",
    "# a chalkboard with the equation \\(18 + 23 = 41\\) written on it. The numbers 18 and\n",
    "# 23 are in light blue, and the result 41 is in light green. The equation \\(18 + 23 = 41\\)\n",
    "# is presented on the chalkboard. To solve this, I will add the two numbers on the\n",
    "# left side of the equation: 18 and 23. Adding these together, \\(18 + 23\\), I calculate\n",
    "# that the sum is 41. This matches the number on the right side of the equation,\n",
    "# confirming its correctness.</think>41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "/tmp/ipykernel_4735/2631123105.py:3: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-xvect-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "import torchaudio\n",
    "\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name, \n",
    "    run_opts={\"device\": device}, \n",
    "    savedir=os.path.join(\"./tmp\", spk_model_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speaker_embedding(audio_file):\n",
    "    \"\"\"从上传的音频文件中提取说话人特征向量\"\"\"\n",
    "    # 加载音频文件\n",
    "    if isinstance(audio_file, tuple):\n",
    "        sample_rate, waveform = audio_file\n",
    "        waveform = torch.tensor(waveform)\n",
    "    else:\n",
    "        waveform, sample_rate = torchaudio.load(audio_file)\n",
    "    \n",
    "    # 确保音频是单声道的\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0)\n",
    "    \n",
    "    # 重采样到16kHz（如果需要）\n",
    "    if sample_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # 确保波形是 [batch, time] 格式\n",
    "    if len(waveform.shape) == 1:\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "    \n",
    "    # 将波形移到正确的设备上\n",
    "    waveform = waveform.to(speaker_model.device)\n",
    "    \n",
    "    # 使用SpeechBrain提取说话人嵌入\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(waveform)\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings = speaker_embeddings.squeeze(1).to(\"cuda\")\n",
    "    \n",
    "    return speaker_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_file = '/home/ubuntu/test.wav'\n",
    "emb = extract_speaker_embedding(audio_file)\n",
    "emb.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0642,  0.0324,  0.0326,  0.0216,  0.0052, -0.0316, -0.0429,\n",
       "           0.0404,  0.0553,  0.0140, -0.0615, -0.0702,  0.0583,  0.0149,\n",
       "           0.0469,  0.0464,  0.0163,  0.0404,  0.0019,  0.0217,  0.0249,\n",
       "           0.0308, -0.0144, -0.0471, -0.0636, -0.0090, -0.0636, -0.0030,\n",
       "           0.0459,  0.0480, -0.0029,  0.0272,  0.0392, -0.0120,  0.0304,\n",
       "          -0.0361,  0.0237,  0.0463,  0.0162, -0.0654,  0.0482,  0.0080,\n",
       "           0.0346,  0.0387,  0.0313, -0.0813, -0.0200,  0.0135, -0.0932,\n",
       "           0.0548,  0.0181,  0.0327,  0.0242,  0.0364, -0.0808, -0.0199,\n",
       "           0.0115,  0.0201,  0.0340,  0.0225,  0.0056, -0.0066, -0.0113,\n",
       "          -0.0148,  0.0274,  0.0656,  0.0374, -0.0467, -0.0651, -0.0464,\n",
       "           0.0293,  0.0078,  0.0184,  0.0083,  0.0252,  0.0376,  0.0190,\n",
       "           0.0253, -0.0715, -0.0929, -0.0880, -0.0374, -0.0708, -0.0834,\n",
       "          -0.0318, -0.0661, -0.0441,  0.0272,  0.0064, -0.0421,  0.0286,\n",
       "          -0.0868,  0.0129, -0.0762,  0.0460, -0.0048,  0.0455,  0.0250,\n",
       "          -0.0803, -0.0785,  0.0311, -0.0870, -0.0621,  0.0170,  0.0835,\n",
       "          -0.0415,  0.0165,  0.0708,  0.0489,  0.0140, -0.0801, -0.0028,\n",
       "           0.0508,  0.0166,  0.0343,  0.0452, -0.0776,  0.0040, -0.0554,\n",
       "          -0.0330,  0.0285, -0.0791,  0.0263,  0.0543, -0.0597,  0.0439,\n",
       "          -0.0893,  0.0371,  0.0306,  0.0291, -0.0097,  0.0149,  0.0139,\n",
       "           0.0499,  0.0114, -0.0876, -0.0909,  0.0133, -0.0554,  0.0110,\n",
       "           0.0188,  0.0025,  0.0136,  0.0499, -0.0901,  0.0198, -0.0080,\n",
       "           0.0012,  0.0581, -0.0636,  0.0288, -0.0433, -0.0494,  0.0261,\n",
       "           0.0519,  0.0370,  0.0115, -0.0597, -0.0879,  0.0448,  0.0237,\n",
       "           0.0152,  0.0160,  0.0244, -0.0252,  0.0036,  0.0537,  0.0329,\n",
       "           0.0252,  0.0295,  0.0103, -0.0596,  0.0276, -0.0505,  0.0146,\n",
       "           0.0215, -0.0609,  0.0095,  0.0184, -0.0781,  0.0289, -0.0854,\n",
       "          -0.0165,  0.0064, -0.0675, -0.0101,  0.0408,  0.0152, -0.0650,\n",
       "          -0.0706, -0.0095,  0.0303, -0.0582,  0.0175,  0.0371,  0.0326,\n",
       "           0.0139, -0.0072,  0.0279, -0.0047,  0.0414,  0.0386, -0.0688,\n",
       "           0.0163,  0.0499,  0.0120,  0.0434, -0.0189,  0.0377,  0.0237,\n",
       "           0.0032, -0.0753, -0.0546,  0.0496, -0.0428,  0.0471,  0.0291,\n",
       "          -0.0997,  0.0346,  0.0400, -0.0764,  0.0140, -0.0828, -0.0121,\n",
       "          -0.0586,  0.0204,  0.0332,  0.0693,  0.0397,  0.0189,  0.0035,\n",
       "           0.0295,  0.0261, -0.0075,  0.0205, -0.0089,  0.0477,  0.0196,\n",
       "           0.0008,  0.0460, -0.0546,  0.0177, -0.0708,  0.0369, -0.0612,\n",
       "           0.0831,  0.0422,  0.0428,  0.0402,  0.0353,  0.0344,  0.0014,\n",
       "           0.0074, -0.0468,  0.0008,  0.0182,  0.0072,  0.0527, -0.0944,\n",
       "           0.0527,  0.0080, -0.0608,  0.0155,  0.0299, -0.0006,  0.0326,\n",
       "           0.0334,  0.0230,  0.0387, -0.0052,  0.0440, -0.0101,  0.0116,\n",
       "           0.0352, -0.0787,  0.0272,  0.0486,  0.0207, -0.0534, -0.0369,\n",
       "          -0.0232,  0.0542,  0.0429, -0.0704,  0.0469,  0.0505,  0.0246,\n",
       "           0.0518, -0.0905,  0.0034,  0.0136,  0.0077,  0.0434, -0.0114,\n",
       "           0.0225,  0.0200,  0.0115, -0.0220,  0.0601,  0.0439, -0.0616,\n",
       "           0.0411,  0.0221, -0.0774,  0.0082,  0.0508,  0.0221,  0.0282,\n",
       "           0.0494,  0.0234,  0.0591,  0.0210, -0.0094,  0.0105, -0.0775,\n",
       "          -0.0627, -0.0200,  0.0290, -0.0619,  0.0510, -0.0323,  0.0212,\n",
       "          -0.0441,  0.0082,  0.0684, -0.0853,  0.0607,  0.0768, -0.0595,\n",
       "          -0.0863, -0.0537, -0.0095,  0.0233,  0.0476, -0.0685,  0.0285,\n",
       "           0.0427,  0.0083,  0.0068,  0.0278,  0.0198, -0.0693,  0.0100,\n",
       "           0.0196, -0.0701,  0.0144,  0.0284,  0.0153, -0.0362, -0.0805,\n",
       "          -0.0537, -0.0207, -0.0529,  0.1125,  0.0312, -0.0437, -0.0060,\n",
       "           0.0396, -0.0380,  0.0298, -0.1045,  0.0005,  0.0131,  0.0122,\n",
       "           0.0316,  0.0444, -0.0656,  0.0098,  0.0071,  0.0509,  0.0072,\n",
       "           0.0494, -0.0634,  0.0458, -0.0128, -0.0015,  0.0229,  0.0211,\n",
       "           0.0432,  0.0344, -0.0945, -0.1192,  0.0481,  0.0029,  0.0325,\n",
       "           0.0468,  0.0522, -0.0705,  0.0357,  0.0234,  0.0428,  0.0244,\n",
       "          -0.0445,  0.0319,  0.0007,  0.0217,  0.0315, -0.0736,  0.0101,\n",
       "           0.0082,  0.0187, -0.0486,  0.0214,  0.0281,  0.0173,  0.0071,\n",
       "           0.0042, -0.0795, -0.0476, -0.0649, -0.0449,  0.0244,  0.0258,\n",
       "          -0.0095,  0.0524, -0.0704, -0.0041, -0.0582, -0.1501, -0.0003,\n",
       "          -0.0552, -0.0006,  0.0300,  0.0102, -0.0674,  0.0400,  0.0562,\n",
       "           0.0428,  0.0296, -0.0537, -0.0447, -0.0092, -0.0200,  0.0145,\n",
       "           0.0437,  0.0108,  0.0535, -0.0024,  0.0315, -0.1002,  0.0074,\n",
       "           0.0161,  0.0370,  0.0013, -0.0115,  0.0295,  0.0435,  0.0052,\n",
       "           0.0097, -0.0062,  0.0470,  0.0373,  0.0274,  0.0493,  0.0131,\n",
       "           0.0112,  0.0279, -0.0742, -0.0722,  0.0325, -0.0115,  0.0154,\n",
       "           0.0371,  0.0426, -0.0688,  0.0532,  0.0197, -0.0137,  0.0328,\n",
       "           0.0373, -0.0022,  0.0382,  0.0399,  0.0042, -0.0386, -0.0562,\n",
       "           0.0172, -0.0058,  0.0270,  0.0406, -0.0648, -0.0390,  0.0424,\n",
       "           0.0253,  0.0350,  0.0601, -0.0468,  0.0516,  0.0135, -0.0090,\n",
       "          -0.0648,  0.0115, -0.0561,  0.0169, -0.0052,  0.0153, -0.0447,\n",
       "          -0.0061, -0.0713, -0.0727,  0.0074,  0.0557, -0.0592,  0.0430,\n",
       "           0.0324, -0.0160, -0.0105,  0.0419,  0.0625, -0.0232,  0.0418,\n",
       "          -0.0409]]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]),\n",
       " tensor([[-7.5731e-02, -2.7370e-02,  1.4933e-02,  4.5861e-02,  8.3840e-03,\n",
       "          -2.7535e-02, -5.1030e-02, -6.1435e-02,  1.4576e-02,  1.9632e-02,\n",
       "          -7.7323e-02, -7.8355e-02,  5.8233e-02,  3.7577e-02,  1.4377e-02,\n",
       "           1.7147e-02, -1.3966e-02,  1.3549e-03,  9.4501e-03,  9.6230e-03,\n",
       "           3.8752e-02,  2.5284e-03, -1.5207e-02, -4.5730e-02, -7.0040e-02,\n",
       "          -8.4035e-03, -5.4758e-02,  4.7528e-03,  5.4306e-02,  1.8867e-02,\n",
       "          -2.7039e-03,  2.1273e-02,  3.8547e-02, -4.7406e-02,  1.2328e-02,\n",
       "          -6.9829e-02,  2.7079e-02,  5.5035e-02, -6.0107e-02, -6.4483e-02,\n",
       "           6.6905e-03, -5.0482e-02,  4.0781e-02,  3.7543e-03,  3.2528e-02,\n",
       "          -1.2350e-01, -1.8370e-02,  1.1340e-02, -5.8363e-02,  4.8635e-02,\n",
       "           1.9366e-02,  3.4130e-02,  2.4126e-02,  1.6346e-02, -8.6061e-02,\n",
       "           2.4379e-03,  1.1567e-02,  3.0723e-02,  3.2634e-02,  1.7792e-02,\n",
       "           3.7105e-02, -1.0155e-02, -1.4131e-02,  4.0054e-02,  2.5886e-03,\n",
       "           2.7985e-02,  1.8849e-02, -3.4497e-02, -6.4935e-02, -6.7838e-02,\n",
       "           1.6550e-02,  1.0723e-02, -5.7267e-02,  1.6395e-02,  2.6811e-02,\n",
       "           1.1489e-02,  2.2631e-04,  3.3649e-02, -4.3076e-02, -1.0138e-01,\n",
       "          -3.2313e-02, -3.9939e-02, -4.8604e-02, -7.3076e-02, -3.2598e-02,\n",
       "          -5.5016e-02, -8.0497e-02,  2.1404e-02,  1.2595e-02,  8.1013e-02,\n",
       "           2.8438e-02, -9.1571e-02,  2.6646e-02, -5.6353e-02,  3.2727e-02,\n",
       "          -6.1148e-03,  2.7991e-02,  2.6409e-02, -5.6170e-02, -7.6057e-02,\n",
       "           3.3457e-02, -2.2366e-02, -6.4855e-02,  2.4247e-02,  2.9651e-02,\n",
       "          -6.8479e-02,  2.7550e-02,  3.8546e-02,  2.3242e-02,  1.2493e-02,\n",
       "          -7.7477e-02,  5.7438e-02,  6.9855e-02,  7.3926e-03,  3.0671e-02,\n",
       "           6.4227e-02, -6.5348e-02, -2.0353e-02, -7.7277e-02, -2.4609e-02,\n",
       "           8.1601e-03, -7.8592e-02,  2.7010e-02,  9.2486e-03, -6.1651e-02,\n",
       "           3.7681e-02, -1.0725e-01,  1.1313e-02,  3.1405e-02,  2.7854e-02,\n",
       "          -4.8992e-03,  2.5332e-02,  9.7699e-03,  5.1193e-02,  1.5345e-02,\n",
       "          -9.4362e-02, -5.6592e-02, -8.8629e-03, -7.1800e-02, -6.0420e-02,\n",
       "           4.4344e-02,  5.5215e-03, -3.5413e-02,  3.9511e-02, -6.7250e-02,\n",
       "           4.5436e-02,  1.6532e-02, -3.8523e-02,  4.5966e-02, -7.9121e-02,\n",
       "           5.2795e-02, -3.1010e-02, -6.3569e-02,  4.1343e-02,  3.5959e-02,\n",
       "           1.1169e-02,  1.6097e-02, -6.2336e-02, -7.2390e-02,  3.0439e-02,\n",
       "           3.6406e-02, -1.8986e-02,  2.1501e-02, -7.7191e-02,  1.6157e-02,\n",
       "           1.1470e-02,  4.5957e-02,  3.3363e-02,  2.3475e-02,  3.0637e-02,\n",
       "           3.1186e-02, -6.5670e-02,  4.1199e-02, -3.9888e-02, -2.1472e-02,\n",
       "           3.0891e-03, -4.2949e-02,  3.0467e-02,  1.6949e-02, -5.1466e-02,\n",
       "           2.5883e-02, -5.3718e-02,  3.5316e-03,  4.0631e-02, -5.4173e-02,\n",
       "          -8.6686e-03,  3.5498e-02,  3.0202e-02, -4.0061e-02, -6.6584e-02,\n",
       "           1.5401e-02,  2.1390e-02, -5.5715e-02,  4.6001e-02,  1.8156e-02,\n",
       "           3.0570e-02,  1.1719e-02, -9.9191e-03,  5.2008e-02, -1.6743e-02,\n",
       "           3.6873e-02,  1.7945e-02, -5.2712e-02,  5.1492e-03,  3.1717e-02,\n",
       "           1.6345e-02,  2.8038e-02, -9.1193e-03,  4.7984e-02, -8.1365e-02,\n",
       "           2.2815e-03, -3.7835e-02, -5.1607e-02,  5.4826e-02, -5.5837e-02,\n",
       "           2.9429e-02,  4.3061e-02, -7.3889e-02,  2.4673e-02,  4.0498e-02,\n",
       "          -6.3633e-02,  2.6250e-02, -7.6712e-02, -3.9236e-02,  4.1200e-02,\n",
       "           5.9500e-02,  3.3125e-02,  8.4764e-02,  2.6894e-02,  2.1208e-02,\n",
       "          -1.9036e-02,  3.6105e-02,  1.1080e-02, -3.6135e-02,  1.6011e-02,\n",
       "          -1.8170e-02,  2.1346e-02,  1.3552e-02, -1.8920e-03,  8.3177e-02,\n",
       "          -4.1955e-02,  2.7798e-02, -8.2717e-02,  3.2804e-02, -7.2306e-02,\n",
       "           4.3091e-02,  2.3945e-02,  2.4701e-02,  1.3977e-02,  5.6885e-02,\n",
       "           1.7945e-02,  2.8972e-02,  8.6228e-03, -5.6302e-02, -1.4323e-02,\n",
       "           2.9686e-02,  5.8318e-03,  3.8803e-02, -4.1574e-02,  4.0746e-02,\n",
       "           3.1213e-02, -8.4970e-02, -2.3686e-03,  3.6791e-02,  1.2361e-03,\n",
       "           4.5098e-02,  5.6538e-03,  2.8413e-02,  1.0165e-02, -5.3070e-02,\n",
       "           2.1911e-02, -1.1162e-02,  3.8611e-02, -3.0208e-02, -5.9430e-02,\n",
       "           3.6008e-02,  1.1929e-02,  3.1447e-02, -1.0400e-01, -6.1293e-02,\n",
       "          -8.0308e-03,  4.2027e-02,  3.0025e-02, -7.4196e-02, -5.2481e-02,\n",
       "           5.1548e-02,  1.8361e-02,  6.1537e-02, -8.1845e-02,  4.4316e-03,\n",
       "           6.7739e-03, -9.4557e-03, -5.6989e-02,  2.4892e-02, -3.0561e-02,\n",
       "           2.5083e-02,  1.9040e-02,  4.1483e-02,  5.1471e-02,  3.7162e-02,\n",
       "           5.9241e-02,  1.8279e-02,  1.9296e-02, -6.7110e-02,  2.0166e-03,\n",
       "           4.8967e-02,  5.0391e-02,  1.3446e-02,  1.8206e-02, -8.5504e-03,\n",
       "           1.1178e-02,  2.0903e-02, -4.1387e-02,  5.5261e-02, -9.4853e-02,\n",
       "          -4.2134e-02, -2.7247e-02,  2.9106e-02, -6.6454e-02,  6.7363e-02,\n",
       "          -3.8123e-02,  2.7640e-02, -3.7964e-02,  1.0054e-02,  5.8676e-02,\n",
       "          -7.0155e-02,  4.6831e-02,  7.9704e-02, -1.0691e-01, -8.5176e-02,\n",
       "          -5.0247e-02,  4.3429e-02,  7.7549e-05,  3.5321e-02, -8.3261e-02,\n",
       "           3.6600e-02,  3.6176e-02, -4.2300e-02, -1.0296e-02, -5.7964e-02,\n",
       "           3.0171e-02, -6.0402e-02,  2.4129e-02, -9.7594e-03, -7.0489e-02,\n",
       "          -5.9752e-03,  1.0908e-02,  3.8074e-02, -2.1607e-02, -6.5632e-02,\n",
       "          -5.6591e-02, -3.7202e-02, -3.5643e-02,  1.1503e-01,  4.8548e-02,\n",
       "          -4.4165e-02, -8.4903e-03,  5.2511e-02, -6.6306e-02, -3.7650e-03,\n",
       "          -9.6806e-02,  6.3695e-03, -4.7472e-03,  8.2472e-03,  3.9955e-02,\n",
       "          -1.8661e-02, -5.2143e-02,  1.0988e-02,  1.8065e-02,  4.5972e-02,\n",
       "           1.2286e-02,  7.0595e-02, -8.4777e-02, -1.4104e-02, -1.3229e-02,\n",
       "          -2.5280e-03,  3.9428e-02,  8.6427e-03,  5.7508e-02,  2.3728e-02,\n",
       "          -6.6087e-02, -1.1664e-01,  2.2290e-02,  5.0236e-02,  2.4568e-02,\n",
       "          -6.6172e-02,  2.6201e-02, -5.5484e-02,  4.6974e-02,  3.0781e-02,\n",
       "           2.4634e-02,  3.1654e-04, -4.1335e-02,  2.2777e-02,  4.0515e-02,\n",
       "          -1.4501e-03,  3.3753e-02, -6.3499e-02, -2.4656e-02, -1.9639e-02,\n",
       "          -1.4018e-02, -6.5658e-02,  4.1642e-03, -3.2586e-02,  1.7866e-02,\n",
       "           7.4335e-03,  6.4375e-04, -5.8995e-02, -6.5919e-02, -8.0942e-02,\n",
       "          -3.7122e-02,  2.8975e-02,  6.9335e-04, -1.5037e-02,  1.6579e-02,\n",
       "          -6.5062e-02,  1.9240e-02, -8.5884e-02, -1.2970e-01, -2.1583e-02,\n",
       "          -4.2093e-02,  9.9256e-03,  2.9919e-02, -5.6503e-02, -5.9666e-02,\n",
       "           3.0643e-02,  6.5749e-02,  5.1784e-02,  3.0791e-02, -3.2522e-02,\n",
       "          -4.9952e-02, -1.8215e-02, -2.5078e-02,  3.5772e-02,  4.6380e-02,\n",
       "           4.1542e-03,  3.4328e-02, -7.7166e-03, -8.1652e-02, -6.3338e-02,\n",
       "           1.9106e-02, -2.6073e-04,  3.7431e-02,  5.0535e-03,  2.1157e-02,\n",
       "           2.8372e-02,  3.9707e-02,  1.0605e-02,  3.4675e-03, -1.4806e-02,\n",
       "           3.8775e-02,  2.4118e-02,  6.7608e-02,  4.9226e-02,  1.0720e-02,\n",
       "           1.3325e-02,  2.7272e-02, -6.4426e-02, -5.2507e-02,  3.4602e-02,\n",
       "          -1.4481e-02,  2.2297e-02,  3.4668e-02,  3.1350e-02, -5.9222e-02,\n",
       "           5.2553e-02,  4.2786e-02, -5.9978e-02,  2.3348e-02,  2.5155e-02,\n",
       "          -2.1993e-03,  3.6892e-02,  2.5243e-02,  4.0611e-03, -5.3466e-03,\n",
       "          -7.2841e-02,  1.3622e-04, -5.5694e-03,  3.9565e-02,  2.9341e-02,\n",
       "          -6.4077e-02, -5.2649e-02, -1.3348e-02,  2.1251e-02,  2.4448e-02,\n",
       "           3.3583e-02, -6.7184e-02,  2.9360e-02, -5.0579e-03, -1.2123e-02,\n",
       "          -4.8513e-02,  2.4554e-02, -5.2060e-02,  2.0694e-02, -3.5575e-02,\n",
       "           1.4269e-02, -4.6220e-02,  3.6354e-02, -8.2248e-02, -6.0916e-02,\n",
       "           1.0230e-02,  3.5427e-02, -6.8191e-02,  2.3711e-02, -3.5071e-02,\n",
       "          -1.2789e-02, -8.0640e-02,  3.7140e-02,  8.2634e-02,  2.2695e-02,\n",
       "           4.4507e-02, -5.3924e-02]], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载声音相似度嵌入向量（可选）\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0).to(\"cuda\")\n",
    "speaker_embeddings.shape,speaker_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_english_words(text):\n",
    "    \"\"\"将文本中的数字转换为英文单词\"\"\"\n",
    "    \n",
    "    def int_to_english(n):\n",
    "        ones = ['', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine',\n",
    "                'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen',\n",
    "                'seventeen', 'eighteen', 'nineteen']\n",
    "        tens = ['', '', 'twenty', 'thirty', 'forty', 'fifty', 'sixty', 'seventy', 'eighty', 'ninety']\n",
    "        \n",
    "        def helper(num):\n",
    "            if num == 0:\n",
    "                return ''\n",
    "            elif num < 20:\n",
    "                return ones[num]\n",
    "            elif num < 100:\n",
    "                return tens[num // 10] + (' ' + ones[num % 10] if num % 10 != 0 else '')\n",
    "            elif num < 1000:\n",
    "                return ones[num // 100] + ' hundred' + (' and ' + helper(num % 100) if num % 100 != 0 else '')\n",
    "            elif num < 1000000:\n",
    "                return helper(num // 1000) + ' thousand' + (' ' + helper(num % 1000) if num % 1000 != 0 else '')\n",
    "            elif num < 1000000000:\n",
    "                return helper(num // 1000000) + ' million' + (' ' + helper(num % 1000000) if num % 1000000 != 0 else '')\n",
    "            else:\n",
    "                return helper(num // 1000000000) + ' billion' + (' ' + helper(num % 1000000000) if num % 1000000000 != 0 else '')\n",
    "    \n",
    "    def convert_number(match):\n",
    "        number = match.group()\n",
    "        # 如果是小数\n",
    "        if '.' in number:\n",
    "            integer_part, decimal_part = number.split('.')\n",
    "            # 处理整数部分\n",
    "            if int(integer_part) == 0:\n",
    "                integer_english = 'zero'\n",
    "            else:\n",
    "                integer_english = int_to_english(int(integer_part))\n",
    "            # 处理小数部分\n",
    "            decimal_english = ' point ' + ' '.join(int_to_english(int(d)) for d in decimal_part)\n",
    "            return integer_english + decimal_english\n",
    "        else:\n",
    "            # 整数处理\n",
    "            num = int(number)\n",
    "            if num == 0:\n",
    "                return 'zero'\n",
    "            return int_to_english(num)\n",
    "    \n",
    "    import re\n",
    "    # 匹配独立的数字（包括小数），但不匹配日期、时间等特殊格式\n",
    "    pattern = r'\\b\\d+\\.?\\d*\\b'\n",
    "    return re.sub(pattern, convert_number, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forty-one\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import inflect\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "    p = inflect.engine()\n",
    "    \n",
    "    def replace_match(match):\n",
    "        return p.number_to_words(int(match.group()))  # 将匹配到的数字转换成英文\n",
    "    \n",
    "    return re.sub(r'\\d+', replace_match, text)  # 查找并替换所有数字\n",
    "\n",
    "# 示例\n",
    "text = \"41\"\n",
    "converted_text = replace_numbers_with_words(text)\n",
    "print(converted_text)  # 输出: I have two apples and fifteen oranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He bought three books for twenty-five dollars.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from num2words import num2words\n",
    "\n",
    "def replace_numbers_with_words(text):\n",
    "    def replace_match(match):\n",
    "        return num2words(int(match.group()))  # 转换为英文单词\n",
    "    \n",
    "    return re.sub(r'\\d+', replace_match, text)\n",
    "\n",
    "# 示例\n",
    "text = \"He bought 3 books for 25 dollars.\"\n",
    "converted_text = replace_numbers_with_words(text)\n",
    "print(converted_text)  # 输出: He bought three books for twenty-five dollars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
