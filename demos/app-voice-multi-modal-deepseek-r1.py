import gradio as gr
import torch
from transformers import (
    AutoProcessor, 
    LlavaForConditionalGeneration, 
    WhisperProcessor,
    WhisperForConditionalGeneration
)
from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, SpeechT5FeatureExtractor
from PIL import Image
import numpy as np
import io
from pydub import AudioSegment
from dotenv import load_dotenv
from datasets import load_dataset
import os
import torchaudio
from speechbrain.pretrained import EncoderClassifier
import re
import inflect

load_dotenv()

# åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«æ¨¡å‹
asr_model_id = "openai/whisper-medium"
asr_processor = WhisperProcessor.from_pretrained(asr_model_id)
asr_model = WhisperForConditionalGeneration.from_pretrained(asr_model_id).to("cuda")
asr_model.config.forced_decoder_ids = asr_processor.get_decoder_prompt_ids(language="chinese", task="transcribe")

# åˆå§‹åŒ–å¤šæ¨¡æ€å¯¹è¯æ¨¡å‹å’Œå¤„ç†å™¨
model_id = "/home/ubuntu/Align-DS-V/"
model = LlavaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
).to("cuda")
processor = AutoProcessor.from_pretrained(model_id)

# åˆå§‹åŒ–å¤šè¯­è¨€è¯­éŸ³åˆæˆæ¨¡å‹
processor_tts = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")
model_tts = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts").to("cuda")
vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan").to("cuda")

# åŠ è½½å£°éŸ³ç›¸ä¼¼åº¦åµŒå…¥å‘é‡ï¼ˆå¯é€‰ï¼‰
embeddings_dataset = load_dataset("Matthijs/cmu-arctic-xvectors", split="validation")
speaker_embeddings = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0).to("cuda")

# åˆå§‹åŒ–è¯´è¯äººè¯†åˆ«æ¨¡å‹
spk_model_name = "speechbrain/spkrec-xvect-voxceleb"
speaker_model = EncoderClassifier.from_hparams(
    source=spk_model_name, 
    run_opts={"device": "cuda" if torch.cuda.is_available() else "cpu"}, 
    savedir=os.path.join("./tmp", spk_model_name)
)

def audio_to_text(audio):
    # æ£€æŸ¥éŸ³é¢‘è¾“å…¥æ˜¯å¦ä¸ºç©º
    if audio is None:
        return "è¯·å…ˆå½•åˆ¶éŸ³é¢‘"
    
    # å°†éŸ³é¢‘è½¬æ¢ä¸ºæ­£ç¡®çš„é‡‡æ ·ç‡
    sample_rate = audio[0]
    audio_data = audio[1]
    
    # ç¡®ä¿éŸ³é¢‘æ˜¯å•å£°é“çš„
    if len(audio_data.shape) > 1:
        audio_data = np.mean(audio_data, axis=0)
    
    # å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºfloat32æ ¼å¼ï¼Œå¹¶å½’ä¸€åŒ–åˆ°[-1, 1]èŒƒå›´
    if audio_data.dtype == np.int16:
        audio_data = audio_data.astype(np.float32) / 32768.0
    
    # é‡é‡‡æ ·åˆ°16kHz
    if sample_rate != 16000:
        # è®¡ç®—é‡é‡‡æ ·æ¯”ä¾‹
        resample_ratio = 16000 / sample_rate
        new_length = int(len(audio_data) * resample_ratio)
        audio_data = np.interp(
            np.linspace(0, len(audio_data)-1, new_length),
            np.arange(len(audio_data)),
            audio_data
        )
    
    # å‡†å¤‡æ¨¡å‹è¾“å…¥
    inputs = asr_processor(
        audio_data,
        sampling_rate=16000,  # å›ºå®šä½¿ç”¨16kHzé‡‡æ ·ç‡
        return_tensors="pt"
    ).to("cuda")
    
    # ç”Ÿæˆè½¬å½•
    with torch.no_grad():
        predicted_ids = asr_model.generate(**inputs)
    transcription = asr_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    
    return transcription

def extract_assistant_response(response):
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
    if not response or not isinstance(response, list) or not response[0]:
        return ""
    
    # è·å–å¯¹è¯å†…å®¹
    conversation = response[0]
    
    # æŸ¥æ‰¾Assistantå›å¤çš„èµ·å§‹å’Œç»“æŸä½ç½®
    assistant_start = conversation.find("<ï½œAssistantï½œ>")
    
    # å¦‚æœæ‰¾ä¸åˆ°Assistantæ ‡è®°ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
    if assistant_start == -1:
        return ""
    
    # æå–Assistantçš„å›å¤ï¼ˆå»æ‰æ ‡è®°ï¼‰
    assistant_response = conversation[assistant_start + len("<ï½œAssistantï½œ>"):]
    
    return assistant_response.strip()

def extract_thinking_process(response):
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
    if not response or not isinstance(response, str):
        return ""
    
    # æŸ¥æ‰¾æ€è€ƒè¿‡ç¨‹çš„èµ·å§‹å’Œç»“æŸä½ç½®
    think_start = response.find("<think>")
    think_end = response.find("</think>")
    
    # å¦‚æœæ‰¾ä¸åˆ°thinkæ ‡è®°ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
    if think_start == -1 or think_end == -1:
        return ""
    
    # æå–æ€è€ƒè¿‡ç¨‹ï¼ˆå»æ‰æ ‡è®°ï¼‰
    thinking_process = response[think_start + len("<think>"):think_end]
    
    return thinking_process.strip()

def extract_speaker_embedding(audio_file):
    """ä»ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶ä¸­æå–è¯´è¯äººç‰¹å¾å‘é‡"""
    try:
        # åŠ è½½éŸ³é¢‘æ–‡ä»¶
        if isinstance(audio_file, tuple):
            sample_rate, waveform = audio_file
            waveform = torch.tensor(waveform)
        else:
            waveform, sample_rate = torchaudio.load(audio_file)
        
        # ç¡®ä¿éŸ³é¢‘æ˜¯å•å£°é“çš„
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0)
        
        # é‡é‡‡æ ·åˆ°16kHzï¼ˆå¦‚æœéœ€è¦ï¼‰
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(sample_rate, 16000)
            waveform = resampler(waveform)
        
        # ç¡®ä¿æ³¢å½¢æ˜¯ [batch, time] æ ¼å¼
        if len(waveform.shape) == 1:
            waveform = waveform.unsqueeze(0)
        
        # å°†æ³¢å½¢ç§»åˆ°æ­£ç¡®çš„è®¾å¤‡ä¸Š
        waveform = waveform.to(speaker_model.device)
        
        # ä½¿ç”¨SpeechBrainæå–è¯´è¯äººåµŒå…¥
        with torch.no_grad():
            speaker_embeddings = speaker_model.encode_batch(waveform)
            speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)
            speaker_embeddings = speaker_embeddings.squeeze(1).to("cuda")
        
        return speaker_embeddings
    except Exception as e:
        print(f"æå–è¯´è¯äººç‰¹å¾æ—¶å‡ºé”™: {str(e)}")
        return None

def number_to_english_words(text):
    """å°†æ–‡æœ¬ä¸­çš„æ•°å­—è½¬æ¢ä¸ºè‹±æ–‡å•è¯"""
    
    p = inflect.engine()
    
    def replace_match(match):
        return p.number_to_words(int(match.group()))  # å°†åŒ¹é…åˆ°çš„æ•°å­—è½¬æ¢æˆè‹±æ–‡
    
    return re.sub(r'\d+', replace_match, text)  # æŸ¥æ‰¾å¹¶æ›¿æ¢æ‰€æœ‰æ•°å­—

def process_input(audio, image, text_input, custom_voice=None):
    # è¾“å…¥éªŒè¯
    if image is None:
        return "è¯·å…ˆä¸Šä¼ å›¾ç‰‡", "è¯·å…ˆä¸Šä¼ å›¾ç‰‡åå†å¼€å§‹å¯¹è¯", "", None
    
    # è·å–è¾“å…¥æ–‡æœ¬ï¼ˆä¼˜å…ˆä½¿ç”¨è¯­éŸ³è¾“å…¥ï¼‰
    if audio is not None:
        text = audio_to_text(audio)
    elif text_input.strip():
        text = text_input.strip()
    else:
        return "è¯·è¾“å…¥æ–‡æœ¬æˆ–å½•åˆ¶è¯­éŸ³", "è¯·æä¾›è¾“å…¥åå†å¼€å§‹å¯¹è¯", "", None
        
    # å‡†å¤‡æ¨¡å‹è¾“å…¥
    conversation = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": text},
                {"type": "image"},
            ],
        },
    ]
    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
    
    # å¤„ç†å›¾åƒå’Œæ–‡æœ¬
    inputs = processor(images=Image.open(image), text=prompt, return_tensors='pt').to("cuda", torch.float16)
    output = model.generate(**inputs, max_new_tokens=4096, do_sample=False)
    response_text = processor.decode(output[0], skip_special_tokens=True)
    
    # æå–æ€è€ƒè¿‡ç¨‹å’ŒAssistantçš„å›å¤
    thinking_process = extract_thinking_process(response_text)
    assistant_response = extract_assistant_response([response_text])
    
    # æ–‡æœ¬è½¬è¯­éŸ³ä½¿ç”¨å¤„ç†åçš„å›å¤
    final_response = assistant_response.replace(f"<think>{thinking_process}</think>", "").strip()
    
    # å°†æ•°å­—è½¬æ¢ä¸ºè‹±æ–‡å•è¯
    final_response = number_to_english_words(final_response)
    final_response = final_response.replace("-", " ")
    
    inputs = processor_tts(text=final_response, return_tensors="pt", padding=True, truncation=True, max_length=550).to("cuda")
    
    try:
        # ä½¿ç”¨è‡ªå®šä¹‰éŸ³è‰²æˆ–é»˜è®¤éŸ³è‰²
        current_speaker_embeddings = extract_speaker_embedding(custom_voice) if custom_voice is not None else speaker_embeddings
        
        if current_speaker_embeddings is None:
            current_speaker_embeddings = speaker_embeddings
            
        speech = model_tts.generate_speech(inputs["input_ids"], current_speaker_embeddings, vocoder=vocoder)
        speech = speech.cpu().numpy()
        
        # ç¡®ä¿éŸ³é¢‘æ•°æ®æ˜¯æœ‰æ•ˆçš„
        if np.isnan(speech).any() or np.isinf(speech).any():
            raise ValueError("ç”Ÿæˆçš„éŸ³é¢‘æ•°æ®åŒ…å«æ— æ•ˆå€¼")
            
        # æ ‡å‡†åŒ–éŸ³é¢‘æ•°æ®
        if speech.size > 0:
            speech = speech / np.max(np.abs(speech))
        
    except Exception as e:
        print(f"è¯­éŸ³ç”Ÿæˆå‡ºé”™: {str(e)}")
        sample_rate = 24000
        duration = 1
        speech = np.zeros(sample_rate * duration)
    
    return text, final_response, thinking_process, (16000, speech)

with gr.Blocks(theme=gr.themes.Soft(
    primary_hue="blue",
    secondary_hue="indigo",
    neutral_hue="slate",
    spacing_size="sm",
    radius_size="lg",
    font=["Source Sans Pro", "ui-sans-serif", "system-ui"]
)) as demo:
    gr.Markdown("""
    # ğŸ¤– R1-Jarvis: æ™ºèƒ½è§†è§‰å¯¹è¯åŠ©æ‰‹

    æ¬¢è¿ä½¿ç”¨åŸºäºDeepseek-R1çš„å¤šæ¨¡æ€è§†è§‰é—®ç­”ç³»ç»Ÿï¼

    ## âœ¨ åŠŸèƒ½ç‰¹ç‚¹
    - æ”¯æŒè¯­éŸ³è¾“å…¥/æ–‡æœ¬è¾“å…¥
    - æ™ºèƒ½å›¾åƒé—®ç­”
    - æ”¯æŒæ–‡æœ¬/è‡ªå®šä¹‰éŸ³è‰²è¯­éŸ³å›å¤
    - æ”¯æŒå¤šè¯­è¨€å¯¹è¯/R1æ·±åº¦æ€è€ƒ
    - Supported By DeepSeek-R1-Distill Models

    ## âœ¨ Todo
    - æµå¼å®æ—¶å¯¹è¯
    - è™šæ‹Ÿäººç‰©å¯¹è¯
    """)

    with gr.Row():
        with gr.Column(scale=1):
            image_input = gr.Image(
                type="filepath",
                label="ğŸ“· ä¸Šä¼ å›¾ç‰‡",
                elem_id="image-input",
                height=300
            )

        with gr.Column(scale=1):
            with gr.Tab("ğŸ¤ è¯­éŸ³è¾“å…¥"):
                audio_input = gr.Audio(
                    sources=["microphone"],
                    type="numpy",
                    label="å½•åˆ¶è¯­éŸ³",
                    elem_id="audio-input"
                )
            
            with gr.Tab("âŒ¨ï¸ æ–‡æœ¬è¾“å…¥"):
                text_input = gr.Textbox(
                    label="è¾“å…¥æ–‡å­—",
                    placeholder="åœ¨æ­¤è¾“å…¥æ‚¨çš„é—®é¢˜...",
                    lines=3
                )

            with gr.Tab("ğŸµ è‡ªå®šä¹‰éŸ³è‰²"):
                custom_voice_input = gr.Audio(
                    label="ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰",
                    type="filepath",
                    elem_id="custom-voice-input"
                )
                gr.Markdown("""
                #### éŸ³è‰²è®¾ç½®è¯´æ˜
                - ä¸Šä¼ ä¸€æ®µæ¸…æ™°çš„è¯­éŸ³æ¥è‡ªå®šä¹‰AIå›å¤çš„éŸ³è‰²
                - å»ºè®®ä½¿ç”¨5-10ç§’çš„æ¸…æ™°è¯­éŸ³
                - å¦‚æœä¸ä¸Šä¼ åˆ™ä½¿ç”¨é»˜è®¤éŸ³è‰²
                """)

            submit_btn = gr.Button("ğŸš€ å¼€å§‹å¯¹è¯", variant="primary", scale=1)

    with gr.Row():
        with gr.Column():
            gr.Markdown("### ğŸ“ è¾“å…¥è¯†åˆ«ç»“æœ")
            text_output = gr.Textbox(
                label="è¯†åˆ«çš„æ–‡æœ¬",
                elem_id="text-output",
                lines=2
            )

    with gr.Row():
        with gr.Column():
            gr.Markdown("### ğŸ¤” R1åˆ†æè¿‡ç¨‹")
            thinking_output = gr.Textbox(
                label="æ€è€ƒè¿‡ç¨‹",
                elem_id="thinking-output",
                lines=4
            )

        with gr.Column():
            gr.Markdown("### ğŸ’¡ R1æœ€ç»ˆå›å¤")
            response_output = gr.Textbox(
                label="å›å¤å†…å®¹",
                elem_id="response-output",
                lines=4
            )

    with gr.Row():
        gr.Markdown("### ğŸ”Š è¯­éŸ³å›å¤")
        audio_output = gr.Audio(
            label="AIè¯­éŸ³",
            type="numpy",
            elem_id="audio-output"
        )

    submit_btn.click(
        process_input,
        inputs=[audio_input, image_input, text_input, custom_voice_input],
        outputs=[text_output, response_output, thinking_output, audio_output]
    )

if __name__ == "__main__":
    demo.launch()
